{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e93a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc8234d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15316, 13), (54846, 11))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "posts_df = pd.read_csv(DATA_DIR / \"the-reddit-dataset-dataset-posts.csv\")\n",
    "comments_df = pd.read_csv(DATA_DIR / \"the-reddit-dataset-dataset-comments.csv\")\n",
    "\n",
    "posts_df = posts_df[posts_df[\"selftext\"].notna()].copy()\n",
    "comments_df = comments_df[comments_df[\"body\"].notna()].copy()\n",
    "\n",
    "posts_df.shape, comments_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6215242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제\n",
    "def clean_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)\n",
    "    t = re.sub(r\"[^a-zA-Z\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "posts_df[\"clean_text\"] = posts_df[\"selftext\"].apply(clean_text)\n",
    "comments_df[\"clean_text\"] = comments_df[\"body\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "325ba795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 7a766c3f-0f7b-45e2-a691-3cf4ac11d89b)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7529c67c1246699bdc7e8b8db0740f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b834b3099e11446cbec6f9fee3daae65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SentenceTransformer 임베딩\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "post_embeddings = embedder.encode(\n",
    "    posts_df[\"clean_text\"].tolist(),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "comment_embeddings = embedder.encode(\n",
    "    comments_df[\"clean_text\"].tolist(),\n",
    "    show_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(posts_df.columns)\n",
    "# # print(comments_df.columns)\n",
    "\n",
    "# Index(['index', 'type', 'id', 'subreddit.id', 'subreddit.name',\n",
    "#        'subreddit.nsfw', 'created_utc', 'permalink', 'domain', 'url',\n",
    "#        'selftext', 'title', 'score', 'clean_text', 'topic'],\n",
    "#       dtype='object')\n",
    "# Index(['index', 'type', 'id', 'subreddit.id', 'subreddit.name',\n",
    "#        'subreddit.nsfw', 'created_utc', 'permalink', 'body', 'sentiment',\n",
    "#        'score', 'clean_text'],\n",
    "#       dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a576874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 10:13:20,740 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-15 10:13:24,638 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-12-15 10:13:24,639 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-15 10:13:25,203 - BERTopic - Cluster - Completed ✓\n",
      "2025-12-15 10:13:25,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-15 10:13:25,652 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "ff10f619-9a0f-47b7-ad42-1f77e35528b6",
       "rows": [
        [
         "0",
         "-1",
         "4108",
         "-1_to_of_the_and",
         "['to', 'of', 'the', 'and', 'data', 'for', 'that', 'is', 'in', 'on']",
         "['hey i know this question is really basic but i ve spend days trying to find a guide about this and i couldn t so i ve collected the responses and i have them in excel and now i want to use a software maxstats to do statistically analyze it i ve added a picture for the sake of clarity could i just replace all the answers with numerical values in the image i added i tried to show what my questions are like can i just replace times hours with then times hours with and so on if i do this for each question and have values ranging from on each question will the software be able to do correlations statistical tests between them will it know that from this q is different from the from q or is there a better to turn questionnaire type answers into something a statistical analysis software can recognize i d really appreciate some help tutorials online always talk only about the statistics part but they rarely say anything about how to get your survey data ready in the first place', 'i have a project in one of my courses and have been asked to implement a knn algorithm on some healthcare data set this is a very open ended project as you can see and i don t have any experience in going out and actually finding a relevant data set which i can use any time this has happened some data set was provided to me and i could just start working on the implementation i was wondering if someone could provide me with some information on how to get started find a data set that i could work on thank you', 'i m in the process of learning data analytics on my own and need to showoff my skills so i can get a real job i ran across an investigation into food waste last week tonight and wanted to get some relevant data and see if we can alliviate some problems i made up a list of data i need to research and thought you all might be able to point me in the right direction the main data that i m looking for are public reports regarding food waste any kind of info on it i assume non profits would be the best bet but i m sure some cities have come up with their own assessments even homelessness rates literacy rates by city county zip code i also need to get data on primary secondary and tertiary routes that shipping companies use financial assessments from ngos organizations think tanks companies involved would be great i m going to guess major corporations in mid to large cities have donated money or material in some way i don t know financing or how to evaluate financial gains and losses but i know that s going to be a crucial factor in evaluating current programs and designing future ones if someone could point me towards or give a brief overview of the concepts i d be obliged overall my main goal is to collect local and state information on companies that produce food waste with no corporate plan identify areas that suffer from malnourishment and identify trade routes that could efficient transport food waste to areas that could use it i m also going to look at countries policies and historical data to show how potential programs could be beneficial the us is huge and it would take forever to evaluate even cities with populations over k the overall goal is to indentify things where are the primary places that food waste is generated by region state city municipality where are the most vulnerable areas to malnourishment by the previously mentioned areas sizes what programs and policies have worked for the previously mentioned areas of focus how can the successful cases be adopted and how can the failed policies be altered what political and financial backing would be needed in each area i know it s a large project and any help would be appreciated this is something that i plan on seeing through till the end i have a somewhat personal stake in this and it s helping people it s a lot of granular data but i m sure it s out there i m just looking for a pointing finger in the right direction to start this off i need a decent sized project to put on my resume and show my current company that i can do in depth analysis on nuanced subjects but present potential solutions in an objective manner bonus points any current data business analysis that can give me pointers in creating a curriculum to learn the necessary programs and concepts']"
        ],
        [
         "1",
         "0",
         "2411",
         "0_deleted_lawdog_scrubbed_cleaned",
         "['deleted', 'lawdog', 'scrubbed', 'cleaned', 'platform', 'original', 'here', 'post', 'up', 'more']",
         "['deleted', 'deleted', 'deleted']"
        ],
        [
         "2",
         "1",
         "1931",
         "1_removed___",
         "['removed', '', '', '', '', '', '', '', '', '']",
         "['removed', 'removed', 'removed']"
        ],
        [
         "3",
         "2",
         "340",
         "2_companies_stock_company_loan",
         "['companies', 'stock', 'company', 'loan', 'credit', 'crypto', 'bank', 'market', 'currency', 'banks']",
         "['hi amp x b a couple of months ago i found that american credit score brokers services in the us are publishing anonymized datasets about mortgages i wonder if there are similar data sources for the european market especially in germany amp x b from my previous post i want to make a research for my new company and i find that it s really hard to find an anonymized mortgage proposals dataset i suppose the dataset should include features such as date mortgage loan type loan period loan capacity percentages borrowers amount borrowers credit score state interest rate etc do you familiar with this problem i will appreciate if you could help me find any kind of resource', 'i m doing some research on the history of uk companies and am looking for a list of uk companies active between and the top by revenue or by number of employees could work i m looking for company name registration date address email phone number or website would be a big plus companies house free company data product seems perfect but i can t find any historical versions', 'i know only paid databases which provide millions of company profiles around the world for different location industry in addition why i opt for paid sources because every day all data are updated if you need accountant s email phone of any other company you have access to his her business profile for example the global database british b b provider offers company profiles with financial historical data for the last years being able to evaluate a company s creditworthiness check how much money a company can borrow discover if a company is involved in any payment disputes in addition is possible to have the credit check any company view company credit score and suggested credit limit get free companies house documents and the most important thing to have trading addresses with contacts decision makers including function position and email address i appreciate a lot that is possible to download any report in pdf for free and obtain background information on a business including liens judgments and bankruptcies so easy amp x b one more source of financial data for companies is global financial data it offers comprehensive accurate and timely fundamentals data accurate reflection of any year end changes financial statement items including standardized annual and interim statements income statement balance sheet cash flow and supplementary items from footnotes over footnotes data collected to ensure consistent calculation of standardized content per share trailing months and stock performance four industry templates industrial bank insurance and utility industry classification and segment level data amp x b if you are looking for a good financial database for companies firstly check potential vendor how transparent and qualitative data it has tries to have a free demonstration of the platform having the possibility to analyze customer service quality of the prospecting tool platform items for me it works in the best way good luck']"
        ],
        [
         "4",
         "3",
         "205",
         "3_comments_reddit_submissions_comment",
         "['comments', 'reddit', 'submissions', 'comment', 'subreddit', 'posts', 'subreddits', 'will', 'bz', 'files']",
         "['the full reddit submission corpus is now available here bytes compressed sha sum a ab d a b bcc bf ab fefeda c cc b this represents all publicly available reddit submissions from january august several notes on this data data is complete from january thru august partial data is available for years and the reason for this is that the id s used when reddit was just a baby were scattered a bit but i am making an attempt to grab all data from and and will make a supplementary upload for that data once i m satisfied that i ve found all data that is available i have added a key called retrieved on with a unix timestamp for each submission in this dataset if you re doing analysis on scores late august data may still be too young and you may want to wait for the august and september additions that i will make available in october this dataset represents approximately million submission objects with score data author title self text media tags and all other attributes available via the reddit api this dataset will go nicely with the full reddit comment corpus that i released a couple months ago the link id from each comment corresponds to the id key in each of the submission objects in this dataset next steps i will provide monthly updates for both comment data and submission data going forward each new month usually adds over million comments and approximately million submissions this fluctuates a bit also i will split this large file up into individual months in the next few days better reddit search my goal now is to take all of this data and create a usable reddit search function that uses comment data to vastly improve search results reddit s current search generally doesn t do much more than look at keywords in the submission title but the new search i am building will use the approximately billion comments to improve results for instance if someone does a search for einstein the current search will return results where the submission title or self text contain the word einstein using comments the search i am building will be able to see how often einstein is mentioned in the body of comments and weight those submissions accordingly an example of this would be if someone posted a question in r askscience how is the general theory of relativity different than the special theory of relativity many of the comments would contain einstein in the comment bodies thereby making that submission relevant when someone does a search for einstein this is just one of the methods for improving reddit s search function i hope to have a beta search in place in early december if you find this data useful for your research or project please consider making a donation so that i can continue making timely monthly contributions donations help cover server costs time involved etc donations are always much appreciated donation page as always if you have any questions feel free to leave comments', 'introduction as a python user i will be mainly discussing approaches to creating datasets using available python packages however the ideas and packages discussed here can be applied in your language of choice what is a large dataset personally i would consider a dataset of reddit submissions or comments large if it takes or more requests to create submission and comment search requests using the pushshift api return items each so a large dataset could be considered as anything larger than items why at a rate limit of requests per minute in ideal scenarios requests would take an hour to complete usually scenarios are not ideal slow api response times or failed requests getting started pick a data source there are two different options available for getting reddit data directly from reddit from pushshift directly from reddit using data directly from reddit will allow you to have the most up to date submissions and comments and you can query via different reddit feeds hot rising new top the main limitation is that you are unable to query for a specific time window as retrieving historical data is unsupported using the reddit api for python a reddit api wrapper called praw exists which allows you to directly query submissions and comments directly using the reddit api there appears to be some limit on the data returned while using praw making it difficult to create large datasets from my tests subreddit methods such as hot top and new returned at most submissions with a limit of none from pushshift pushshift maintains a database of historical reddit comments and submissions which you can query via the corresponding pushshift api python wrappers also exist for the pushshift api psaw and pmaw made by myself using pushshift in the rest of this post i will be discussing using pushshift via either psaw or pmaw as the ability to query data based on date allows you to compose a large dataset of posts with queries that returns all submissions and comments indexed by pushshift for a specified time period limitations of pushshift there a couple of drawbacks to using pushshift there is only a single maintainer there are a couple of small windows that are missing some number of comments or submissions due to pushshift going down these are rarely backfilled post and comment data can be delayed by up to days making pushshift unrealistic for retrieving the most recent submissions and comments when not to use pushshift you need the most recent submissions or comments from reddit you need less than submissions from any one subreddit picking an api wrapper i m sure there are many api wrappers that exist for pushshift but with regards to python i will only be discussing psaw and pmaw psaw psaw is a great wrapper for interacting with the pushshift api and i have used it in many of my personal projects this package provides generators for the different api endpoints which allows you to work with the data while you are making requests however the main limitation of this library when building a large dataset is the performance i found i was encountering performance issues when trying to request large numbers of posts or comments psaw uses exponential backoff for rate limiting and i found this was the source of my issues as well the paging for id based search is not implemented i ll explain why this is important later pmaw pmaw this is a wrapper i created for building large datasets using the pushshift api this was created to address the performance limitations of psaw at the cost of some other nice to have features such as aggs asc sort and the use of generators pmaw uses multiple threads to make requests and provides different configurations for rate limiting rate averaging exponential backoff with different types of jitter psaw vs pmaw from this benchmark requesting up to submissions we can see that pmaw was on average x faster than psaw when submissions were retrieved the total completion time for submissions with psaw was h m while the average completion time was h m for pmaw creating your dataset now that we ve covered the background and you ve decided to use pmaw to create your dataset i will cover the process for two different types of datasets submissions and comments submissions a dataset of submissions is straightforward to make as all you have to do is use a single pushshift api endpoint to request all your data this can be implemented using the following python code python import pandas as pd from pmaw import pushshiftapi api pushshiftapi submissions api search submissions subreddit wallstreetbets limit sub df pd dataframe submissions sub df to csv data wallstreetbets subs csv header true index false columns list sub df axes comments there are two main ways to approach building a comments dataset first we can directly use the search comments endpoint and return comments based on this search or we can return all comments for a set of submissions approach in this approach we directly search for comments python import pandas as pd from pmaw import pushshiftapi api pushshiftapi comments api search comments subreddit wallstreetbets limit comments df pd dataframe comments comments df to csv data wallstreetbets comments csv header true index false columns list comments df axes approach in this approach we create a dataset of comments based on the submissions we have already retrieved python subs df pd read csv f data wallstreetbets subs csv header sub ids list subs df loc id retrieve comment ids for submissions comment ids api search submission comment ids ids sub ids comment ids list comment ids retrieve comments by id comments api search comments ids comment ids comments df pd dataframe comments comments df to csv data wallstreetbets comments csv header true index false columns list comments df axes links reddit api praw pushshift psaw pmaw', 'the reddit october comment dump rc bz is now ready there are a few changes that i want to discuss along with some other important information about the dataset as you may know the subreddit r incels was banned by reddit earlier this month this happened while i was recrawling comments for october which means once it was banned all comments and posts were no longer available via the reddit api however i also have a real time ingest feed and i usually keep the real time comments for a couple of months before purging them r incels subreddit data after asking the community the general consensus was that it was worth including the r incels comments and posts within my data dumps any comments from r incels that were imported from my real time ingest will have a score of null there is also about a day s worth of comments from that subreddit from october that i ingested from an unrelated project those will have actual scores for the comments just keep in mind that if the score is null it was from the real time ingest this means that the retrieved on epoch timestamp for those comments will be a couple of seconds from the created utc of the comments since i normally ingest comments between seconds after they are made two fields dropped i have also decided to drop two fields from the monthly dumps collapsed and collapsed reason these fields don t really offer any valuable data when a comment gets a score of around it will be collapsed in the normal reddit listing approximately less than of comments in the dumps will have true for collapsed and the reason will state that the score was below that threshold also reddit has added a new field called permalink for comments i am now including them in the monthly dumps but for october some comments have them and some do not going forward all comments in the monthly dumps will have them let me know if you have any questions the link to the file is the file size bytes for the october comment dump is around larger because of the new permalink field there are comments in rc bz an average of comments per second sha edca b d bc b ba aa def fdb b e b f dcd']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4108</td>\n",
       "      <td>-1_to_of_the_and</td>\n",
       "      <td>[to, of, the, and, data, for, that, is, in, on]</td>\n",
       "      <td>[hey i know this question is really basic but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2411</td>\n",
       "      <td>0_deleted_lawdog_scrubbed_cleaned</td>\n",
       "      <td>[deleted, lawdog, scrubbed, cleaned, platform,...</td>\n",
       "      <td>[deleted, deleted, deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1931</td>\n",
       "      <td>1_removed___</td>\n",
       "      <td>[removed, , , , , , , , , ]</td>\n",
       "      <td>[removed, removed, removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>340</td>\n",
       "      <td>2_companies_stock_company_loan</td>\n",
       "      <td>[companies, stock, company, loan, credit, cryp...</td>\n",
       "      <td>[hi amp x b a couple of months ago i found tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>205</td>\n",
       "      <td>3_comments_reddit_submissions_comment</td>\n",
       "      <td>[comments, reddit, submissions, comment, subre...</td>\n",
       "      <td>[the full reddit submission corpus is now avai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                   Name  \\\n",
       "0     -1   4108                       -1_to_of_the_and   \n",
       "1      0   2411      0_deleted_lawdog_scrubbed_cleaned   \n",
       "2      1   1931                           1_removed___   \n",
       "3      2    340         2_companies_stock_company_loan   \n",
       "4      3    205  3_comments_reddit_submissions_comment   \n",
       "\n",
       "                                      Representation  \\\n",
       "0    [to, of, the, and, data, for, that, is, in, on]   \n",
       "1  [deleted, lawdog, scrubbed, cleaned, platform,...   \n",
       "2                        [removed, , , , , , , , , ]   \n",
       "3  [companies, stock, company, loan, credit, cryp...   \n",
       "4  [comments, reddit, submissions, comment, subre...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [hey i know this question is really basic but ...  \n",
       "1                        [deleted, deleted, deleted]  \n",
       "2                        [removed, removed, removed]  \n",
       "3  [hi amp x b a couple of months ago i found tha...  \n",
       "4  [the full reddit submission corpus is now avai...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTopic 학습 (Posts)\n",
    "topic_model_posts = BERTopic(verbose=True)\n",
    "\n",
    "post_topics, _ = topic_model_posts.fit_transform(\n",
    "    posts_df[\"clean_text\"],\n",
    "    post_embeddings\n",
    ")\n",
    "\n",
    "posts_df[\"topic\"] = post_topics\n",
    "topic_model_posts.get_topic_info().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0060c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 10:13:28,855 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-15 10:13:49,090 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-12-15 10:13:49,093 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-15 10:13:54,162 - BERTopic - Cluster - Completed ✓\n",
      "2025-12-15 10:13:54,175 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-15 10:13:55,269 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "f2105199-16a5-4461-9967-32570775794a",
       "rows": [
        [
         "0",
         "-1",
         "19984",
         "-1_amp_that_to_the",
         "['amp', 'that', 'to', 'the', 'of', 'in', 'data', 'it', 'is', 'and']",
         "['i was going to paste some of the links i used but i realize how out of date they might be just start by searching the actual problem you are looking at there are very few unique statistical problems and this is not one of them use information from your search results to further refine your searching once you find some consistency in methods read up on the most common and decide which is the one you should use looks like you ve already decided on two way anova so read about that in detail honestly you re still in school there s never a better time to be wrong', 'do you mean the value of the home or the most recent sales price i don t think you can get sales price at that level plus you d want date sold to be incorporated as well in case the turnover is very low that sounds even more difficult if you mean home value you can get that through the census use factfinder select all the tracts from the state s you want then search for dataset code b home value and pick the year estimates you ll have to download the data transpose it clean up the columns a little but if you re mapping it you can then join that data to a census tract dataset you have or can get from the census as well edit i realized i didn t click your link and that dataset shows median home value which is code b edit also realized you can t view data for more than geographies at a time you ll have to do states at a time then put them all into one spreadsheet there may be a bulk download option though not sure', 'what area are you looking for i went in search of something similar some time ago and i could certainly make use of it again now the data is generally either too aggregated or not aggregated at all so not a whole lot of use what areas did you have in mind i ve got a project i m working on that certainly could use that data but parsing it is a real pain and there s no continuity between jurisdictions if someone else wanted a city or two of data i might be more inclined to try to turn a city s open data into something usable off the top of my head austin and denver have decent open datasets as i recall chicago s is decent but goes back far enough that it s unwieldy to work with then there s the problem where it s data about charges and citations not necessarily whether they were found guilty but that s a whole separate issue a more representative view might be found by working within an x block radius of a given address than by zip code if that s practical why go here and look at it s not even contiguous in reality this is indicative of a fairly serious problem across the country it s not nearly as easy as it should be to say is this home i want to buy in a safe area it s even worse if for instance you don t care about dvs but you do care about robberies because that s your personal risk tolerance']"
        ],
        [
         "1",
         "0",
         "2629",
         "0_deleted_gridmet_meta_saved",
         "['deleted', 'gridmet', 'meta', 'saved', '', '', '', '', '', '']",
         "['deleted', 'deleted', 'deleted']"
        ],
        [
         "2",
         "1",
         "982",
         "1_removed_comment_aw_sticky",
         "['removed', 'comment', 'aw', 'sticky', 'visible', 'loaded', 'gone', 'tags', 'longer', 'remove']",
         "['removed', 'removed', 'removed']"
        ],
        [
         "3",
         "2",
         "906",
         "2_dataset_datasets_link_found",
         "['dataset', 'datasets', 'link', 'found', 'thanks', 'here', 'find', 'sharing', 'share', 'this']",
         "['what s in the dataset', 'what dataset', 'what dataset is this']"
        ],
        [
         "4",
         "3",
         "802",
         "3_lmk_approx_um_mm",
         "['lmk', 'approx', 'um', 'mm', 'answer', 'since', 'time', 'get', 'an', 'so']",
         "['', 'mm', 'if you get an answer can you lmk']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19984</td>\n",
       "      <td>-1_amp_that_to_the</td>\n",
       "      <td>[amp, that, to, the, of, in, data, it, is, and]</td>\n",
       "      <td>[i was going to paste some of the links i used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2629</td>\n",
       "      <td>0_deleted_gridmet_meta_saved</td>\n",
       "      <td>[deleted, gridmet, meta, saved, , , , , , ]</td>\n",
       "      <td>[deleted, deleted, deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>982</td>\n",
       "      <td>1_removed_comment_aw_sticky</td>\n",
       "      <td>[removed, comment, aw, sticky, visible, loaded...</td>\n",
       "      <td>[removed, removed, removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>906</td>\n",
       "      <td>2_dataset_datasets_link_found</td>\n",
       "      <td>[dataset, datasets, link, found, thanks, here,...</td>\n",
       "      <td>[what s in the dataset, what dataset, what dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>802</td>\n",
       "      <td>3_lmk_approx_um_mm</td>\n",
       "      <td>[lmk, approx, um, mm, answer, since, time, get...</td>\n",
       "      <td>[, mm, if you get an answer can you lmk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                           Name  \\\n",
       "0     -1  19984             -1_amp_that_to_the   \n",
       "1      0   2629   0_deleted_gridmet_meta_saved   \n",
       "2      1    982    1_removed_comment_aw_sticky   \n",
       "3      2    906  2_dataset_datasets_link_found   \n",
       "4      3    802             3_lmk_approx_um_mm   \n",
       "\n",
       "                                      Representation  \\\n",
       "0    [amp, that, to, the, of, in, data, it, is, and]   \n",
       "1        [deleted, gridmet, meta, saved, , , , , , ]   \n",
       "2  [removed, comment, aw, sticky, visible, loaded...   \n",
       "3  [dataset, datasets, link, found, thanks, here,...   \n",
       "4  [lmk, approx, um, mm, answer, since, time, get...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [i was going to paste some of the links i used...  \n",
       "1                        [deleted, deleted, deleted]  \n",
       "2                        [removed, removed, removed]  \n",
       "3  [what s in the dataset, what dataset, what dat...  \n",
       "4           [, mm, if you get an answer can you lmk]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTopic 학습 (Comments)\n",
    "topic_model_comments = BERTopic(verbose=True)\n",
    "\n",
    "comment_topics, _ = topic_model_comments.fit_transform(\n",
    "    comments_df[\"clean_text\"],\n",
    "    comment_embeddings\n",
    ")\n",
    "\n",
    "comments_df[\"topic\"] = comment_topics\n",
    "topic_model_comments.get_topic_info().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46ec40b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11208, 15), (34862, 13))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noise(-1) 제거\n",
    "posts_df_valid = posts_df[posts_df[\"topic\"] != -1].copy()\n",
    "comments_df_valid = comments_df[comments_df[\"topic\"] != -1].copy()\n",
    "\n",
    "posts_df_valid.shape, comments_df_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a15023d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10\n",
    "\n",
    "top_topics = (\n",
    "    posts_df_valid[\"topic\"]\n",
    "    .value_counts()\n",
    "    .head(TOP_N)\n",
    "    .index\n",
    ")\n",
    "\n",
    "posts_top = posts_df_valid[posts_df_valid[\"topic\"].isin(top_topics)]\n",
    "comments_top = comments_df_valid[comments_df_valid[\"topic\"].isin(top_topics)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0a7aaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "post_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "comment_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "09a67c72-e498-41c0-89f4-db93614f20d5",
       "rows": [
        [
         "0",
         "0.40843638827714723",
         "0.3061961332401584"
        ],
        [
         "1",
         "0.3271218024733187",
         "0.11437223386908922"
        ],
        [
         "2",
         "0.05759783161104523",
         "0.10552061495457722"
        ],
        [
         "3",
         "0.034728104353718446",
         "0.09340787328208712"
        ],
        [
         "4",
         "0.03337286125698797",
         "0.0803633822501747"
        ],
        [
         "5",
         "0.03218702354734881",
         "0.07058001397624039"
        ],
        [
         "6",
         "0.028290699644248688",
         "0.06790123456790123"
        ],
        [
         "7",
         "0.02727426732170083",
         "0.06580479850920103"
        ],
        [
         "8",
         "0.025919024224970354",
         "0.0506638714185884"
        ],
        [
         "9",
         "0.025071997289513807",
         "0.0451898439319823"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_ratio</th>\n",
       "      <th>comment_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.408436</td>\n",
       "      <td>0.306196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327122</td>\n",
       "      <td>0.114372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057598</td>\n",
       "      <td>0.105521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034728</td>\n",
       "      <td>0.093408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033373</td>\n",
       "      <td>0.080363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.070580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.028291</td>\n",
       "      <td>0.067901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.027274</td>\n",
       "      <td>0.065805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.025919</td>\n",
       "      <td>0.050664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.025072</td>\n",
       "      <td>0.045190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_ratio  comment_ratio\n",
       "topic                           \n",
       "0        0.408436       0.306196\n",
       "1        0.327122       0.114372\n",
       "2        0.057598       0.105521\n",
       "3        0.034728       0.093408\n",
       "4        0.033373       0.080363\n",
       "5        0.032187       0.070580\n",
       "6        0.028291       0.067901\n",
       "7        0.027274       0.065805\n",
       "8        0.025919       0.050664\n",
       "9        0.025072       0.045190"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post vs Comment 토픽 분포 비교\n",
    "post_dist = posts_top[\"topic\"].value_counts(normalize=True)\n",
    "comment_dist = comments_top[\"topic\"].value_counts(normalize=True)\n",
    "\n",
    "topic_compare = pd.concat(\n",
    "    [\n",
    "        post_dist.rename(\"post_ratio\"),\n",
    "        comment_dist.rename(\"comment_ratio\")\n",
    "    ],\n",
    "    axis=1\n",
    ").fillna(0)\n",
    "\n",
    "topic_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82f48fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "subreddit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "93d4e7db-7adf-4213-80b2-ad330802224f",
       "rows": [
        [
         "0",
         "datasets",
         "0",
         "2411"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>topic</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datasets</td>\n",
       "      <td>0</td>\n",
       "      <td>2411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  topic  count\n",
       "0  datasets      0   2411"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subreddit별 대표 토픽\n",
    "posts_sub = posts_df_valid.rename(columns={\"subreddit.name\": \"subreddit\"})\n",
    "posts_sub = posts_sub[posts_sub[\"subreddit\"].notna()]\n",
    "\n",
    "subreddit_top_topic = (\n",
    "    posts_sub.groupby(\"subreddit\")[\"topic\"]\n",
    "    .value_counts()\n",
    "    .groupby(level=0)\n",
    "    .head(1)\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "subreddit_top_topic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "602adba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "examples",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "74caeac5-545a-4ae9-819a-3cabb602dbd4",
       "rows": [
        [
         "0",
         "0",
         "deleted, lawdog, scrubbed, cleaned, platform, original, here, post, up, more",
         "['deleted', 'deleted', 'deleted']"
        ],
        [
         "1",
         "1",
         "removed, , , , , , , , , ",
         "['removed', 'removed', 'removed']"
        ],
        [
         "2",
         "2",
         "companies, stock, company, loan, credit, crypto, bank, market, currency, banks",
         "['i am interested to have a huge list of companies around the globe i have seen crunchbase wikipedia and other websites but none of them grouping a huge list where i can get all of them do you know a place link repository or dataset where i can get that list from the company i am looking for name country and any way to communicate with the business highly appreciate any help on this', 'i am doing some work on stock exchange competition and some information on how listing fees have changed for nyse and nasdaq from to i appreciate any input and help thanks', 'any chance anyone can help i d be forever in debt to you comment if you can help i can dm you my email thanks for your time']"
        ],
        [
         "3",
         "3",
         "comments, reddit, submissions, comment, subreddit, posts, subreddits, will, bz, files",
         "['is there a data set or data base out there that contains comments from social media such as tiktok or instagram amp x b ive just browsed google about it but i haven t found anything', 'hey all i m looking for historical data on subreddit subscriber growth subredditstats com and frontpagemetrics com provide nice graphs showing the subscriber growth of a given subreddit over the years but i didn t find a way to get the data myself does anybody know where to find it where do those two sites get the data from tia', 'trying to do some research on impact of news articles and sentiment on social media alternatively if one is not available is it possible to extract a list of posts from a subreddit such as r coronavirus apparently scraping via the api only gives a max list of submissions which might not be enough thanks r coronavirus']"
        ],
        [
         "4",
         "4",
         "election, political, elections, votes, vote, voting, results, polling, county, state",
         "['want to see detail info of how political people are across different places and cities towns want to see how voting pattern of the same age or age groups compares across cities towns want to see which cities towns are more political and more apathetic looking for any other info that can help see which cities towns are more political and which are more apathetic for presidential election looking for charts that shows the percentage of how each age or age group voted for each city town or state by percentage if you don t know where to find these charts where to find the data want to see which cities towns are more political or more apathetic want to see detail info of voting patterns data of local elections are likely much harder to find yes looking for any other info that can help see which cities towns are more political or more apathetic', 'let s say there is a poll that people voted on the topic and choices with votes in parentheses are as following which brand of tv is better samsung sony same thing how would you interpret the data samsung would be the answer bc it had the most votes or same thing be the answer be it encompasses the two individual options in a singular choice and the two individual options as well as choice are fairly evenly distributed side question how does choice affect votes couldn t i have drawn the conclusion they are about the same just from having choices and polled', 'hello i m looking to find a dataset that shows what percentage of demographics voted towards either major us political party in the past election i ve seen something that looks like this but i was wondering if there was something free that was put up by an official govt source if anyone found something like this please share i d really appreciate it thanks']"
        ],
        [
         "5",
         "5",
         "covid, cases, gt, coronavirus, csv, country, virus, comparison, symptoms, confirmed",
         "['a quick google search of party breakdown of covid deaths suggests more republicans died of covid but the question depends on congressional district and more any data sets to help answer this', 'hi this might be easy to find but for some reason i can t find it looking for a yearly global dataset for covid any help or tips would be greatly appreciated', 'i have been looking for relevant data concerning the shift towards virtual platforms following the pandemic i e working from home or the use of online dating apps for an upcoming school project does anyone know of an open sourced dataset like this hours of searching and no luck']"
        ],
        [
         "6",
         "6",
         "images, image, object, dataset, detection, model, photos, train, of, pictures",
         "['i want to classify if an image contains a contract or not does anyone know a dataset that can help me with that thanks', 'hello i m looking for a dataset to train my model of images of d printed objects with different kinds of defects and problems for my dissertation and i can t find anything if anyone can help me with that it ll be highly appreciated thank you', 'i am planning to embark on a toy project to collect screenshots of various apps on the app store and train a model to recognize common ui elements buttons labels images etc was wondering if there are any practical uses of such a model dataset']"
        ],
        [
         "7",
         "7",
         "music, songs, lt, gt, song, artist, spotify, key, artists, lyrics",
         "['wondering if there is any dataset out there where songs are listed alongside the feeling that they evoke e g songs tagged as calming or happy or motivating i understand that the feeling one gets from music is subjective but i just want to know if anything is out there', 'gt tl dr here hi guys its that time of the day again i published on kaggle a dataset of trending music tracks from tiktok so now you can use your data science skills for becoming a tiktok legend or simply play aound with it just because it is pretty cool the dataset contains trending tracks featured on tiktok including some techincal information like duration author name and the playlist name it also includes some other interesting metrics like the liveness score you can find it here have fun', 'hey for my project i need to build a cnn model to convert sheet music into abc notation i tried to find a good dataset with no luck is there any dataset that will be good enough for this thanks']"
        ],
        [
         "8",
         "8",
         "crime, police, shootings, crimes, gun, mass, violent, fbi, violence, state",
         "['hey guys checkout this new dataset on various crimes in la if you like the dataset make sure to upvote and perform analysis on it also a great starter dataset for beginners as well as intermediates it contains various information about when and at what time the crime was reported as well as victim s age and sex it also has the data about where was the crime committed and what was the crime', 'i submitted a post asking for spatial datasets yesterday but didn t provide enough information for people to give really useful examples i m looking for at least datasets which have the following properties the two datasets cover the same region in part or in full the data in both datasets is in the form of individual points not aggregated over large areas ex states counties or grids either the existence of a point in a location is the entirety of the data provided by a point or a variable gives the value at each point the data should satisfy assumptions for kriging data will be used to demo a framework in r and eventually python which performs kernel density estimation kriging and various forms of interpolation on two datasets and compares the resulting models an example of the type of data i m searching for would be a dataset containing the address can geocode to lat lon at which a crime occurred where each point denotes a crime i tried a dataset of this type for houston but it didn t meet the assumptions for any form of kriging', 'any recent dataset on us crime rate per state could it be found as government open data where would you go for this thanks']"
        ],
        [
         "9",
         "9",
         "weather, temperature, hourly, climate, precipitation, absmaxtemp, avgmintemp, noaa, daily, temperatures",
         "['i m working on a project and am looking for a data source that aggregates historical weather data by city region and can tell me if a region is relatively sunny rainy snowy or hot cold etc not looking for daily monthly data but instead summary data i know i can gather historical noaa weather data and calculate this myself but i m looking for a source that s already done the historical analysis and aggregation anyone know of anything thanks in advance', 'hourly weather data is available from observations but also from models that estimate local weather conditions a bit like weather forecasts for the past i ve compared estimates from oikolab and era to observations from the royal netherlands meteorological institute knmi for three locations in the netherlands', 'this dataset contains temperatures from different cities in canada minimum and maximum temperatures of january winter july summer and through out the year are given in this dataset elevation co ordinates of those cities are also mentioned one can perform geospatial analysis using this data dataset']"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>deleted, lawdog, scrubbed, cleaned, platform, ...</td>\n",
       "      <td>[deleted, deleted, deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>removed, , , , , , , , ,</td>\n",
       "      <td>[removed, removed, removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>companies, stock, company, loan, credit, crypt...</td>\n",
       "      <td>[i am interested to have a huge list of compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>comments, reddit, submissions, comment, subred...</td>\n",
       "      <td>[is there a data set or data base out there th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>election, political, elections, votes, vote, v...</td>\n",
       "      <td>[want to see detail info of how political peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>covid, cases, gt, coronavirus, csv, country, v...</td>\n",
       "      <td>[a quick google search of party breakdown of c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>images, image, object, dataset, detection, mod...</td>\n",
       "      <td>[i want to classify if an image contains a con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>music, songs, lt, gt, song, artist, spotify, k...</td>\n",
       "      <td>[wondering if there is any dataset out there w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>crime, police, shootings, crimes, gun, mass, v...</td>\n",
       "      <td>[hey guys checkout this new dataset on various...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>weather, temperature, hourly, climate, precipi...</td>\n",
       "      <td>[i m working on a project and am looking for a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                           keywords  \\\n",
       "0      0  deleted, lawdog, scrubbed, cleaned, platform, ...   \n",
       "1      1                          removed, , , , , , , , ,    \n",
       "2      2  companies, stock, company, loan, credit, crypt...   \n",
       "3      3  comments, reddit, submissions, comment, subred...   \n",
       "4      4  election, political, elections, votes, vote, v...   \n",
       "5      5  covid, cases, gt, coronavirus, csv, country, v...   \n",
       "6      6  images, image, object, dataset, detection, mod...   \n",
       "7      7  music, songs, lt, gt, song, artist, spotify, k...   \n",
       "8      8  crime, police, shootings, crimes, gun, mass, v...   \n",
       "9      9  weather, temperature, hourly, climate, precipi...   \n",
       "\n",
       "                                            examples  \n",
       "0                        [deleted, deleted, deleted]  \n",
       "1                        [removed, removed, removed]  \n",
       "2  [i am interested to have a huge list of compan...  \n",
       "3  [is there a data set or data base out there th...  \n",
       "4  [want to see detail info of how political peop...  \n",
       "5  [a quick google search of party breakdown of c...  \n",
       "6  [i want to classify if an image contains a con...  \n",
       "7  [wondering if there is any dataset out there w...  \n",
       "8  [hey guys checkout this new dataset on various...  \n",
       "9  [i m working on a project and am looking for a...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topic_keywords(model, topic_id, n=10):\n",
    "    return \", \".join([w for w, _ in model.get_topic(topic_id)[:n]])\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for topic_id in top_topics:\n",
    "    summary_rows.append({\n",
    "        \"topic\": topic_id,\n",
    "        \"keywords\": get_topic_keywords(topic_model_posts, topic_id),\n",
    "        \"examples\": (\n",
    "            posts_df_valid[posts_df_valid[\"topic\"] == topic_id]\n",
    "            .head(3)[\"clean_text\"]\n",
    "            .tolist()\n",
    "        )\n",
    "    })\n",
    "\n",
    "topic_summary_df = pd.DataFrame(summary_rows)\n",
    "topic_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f7886a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df_valid.to_csv(\n",
    "    OUTPUT_DIR / \"posts_with_topics.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "comments_df_valid.to_csv(\n",
    "    OUTPUT_DIR / \"comments_with_topics.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "topic_compare.to_csv(\n",
    "    OUTPUT_DIR / \"post_comment_topic_distribution.csv\"\n",
    ")\n",
    "\n",
    "subreddit_top_topic.to_csv(\n",
    "    OUTPUT_DIR / \"subreddit_top_topics.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "topic_summary_df.to_csv(\n",
    "    OUTPUT_DIR / \"topic_summary_table.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
